FL-14-Sep-2016 Created
FL-17-Dec-2018 Changed

The contents of selected log tables is collected in the table ERROR_STORE. 
In order to create such a table in the links_logs db: 
	$ mysql -u links -p links_logs < ERROR_STORE.schema.sql
Apart from the pk, the table also contains the index: 
	UNIQUE KEY `unique_index` (`id_source`,`location`,`reg_type`,`date`,`sequence`,`role`,`content`)
Its role is to prevent duplicates for the combination of those 7 columns. 

Two Python scripts are used to create the reports: 
	import_logs.py
	Based on the provided date range, The script import_logs.py loads selected 
	cleaning log tables into the ERROR_STORE table. 
	The values in column links_logs.ERROR_STORE.flag are set to 1. 
	
	export_reports.py
	Before running the script export_reports.py, 'manually' set the value in 
	column links_logs.ERROR_STORE.flag to 2, 
	for those records that you want to be included in csv output files. 
	E.g. set all values to 2:
		mysql> UPDATE links_logs.ERROR_STORE SET flag = 2;
	Only records with flag value 2 will be processed by the script. 
	During running the script, the flag value is updated from 2 to 3. 
	The generated csv files are written to the subdirectory csv. 

The scripts require several extra python modules not present in the system python. 
We prefer the use of 'virtual' pythons, to which we added the needed modules, see 
virtual-python-3.5.2.txt. 
In order to automatically activate the virtual python, the 2 python scripts 
are started via 2 tiny shell scripts: 
	$ cd /data/links/clean/export_reports
	$ ./import_logs.sh
and
	$ ./export_reports.sh

In github, you can find these and other scripts in 
links/LinksProject/src/main/python

[eof]
